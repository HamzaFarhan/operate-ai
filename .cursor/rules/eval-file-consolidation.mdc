---
description: 
globs: 
alwaysApply: false
---
# Evaluation File Consolidation Pattern

## Overview
This rule documents the pattern for consolidating evaluation files in [evals/scenario1/](mdc:evals/scenario1) by combining ground truth generation with evaluation dataset creation into a single file per step.

## Target Pattern
Each step should have ONE consolidated file: `stepN_evals.py` that combines:
- Ground truth calculation function
- Query definitions (defined once)
- Dynamic dataset creation using ground truth
- CSV generation
- Evaluation execution

## File Structure Before Consolidation
```
stepN_data_filtering_cohort.py  # Contains ground truth function
stepN_evals.py                  # Contains hardcoded dataset
stepN_evaluation.csv            # Generated CSV with strategy column
```

## File Structure After Consolidation
```
stepN_evals.py                  # Combined file with everything
stepN_evaluation.csv            # Generated CSV without strategy column
```

## Implementation Steps

### 1. Extract Ground Truth Function
From files like [step1_data_filtering_cohort.py](mdc:evals/scenario1/step1_data_filtering_cohort.py):
- Copy the main calculation function (e.g., `get_active_customers_jan_2023()`)
- Ensure it returns structured data for all segments

### 2. Define Queries Dictionary
Create a `QUERIES` dictionary to avoid repetition:
```python
QUERIES = {
    "key1": "Query string for first case...",
    "key2": "Query string for second case...",
    # etc.
}
```

### 3. Dynamic Dataset Creation
Replace hardcoded expected outputs with ground truth calls:
```python
def create_stepN_dataset():
    ground_truth = get_ground_truth_function()
    
    # Use ground_truth data to create expected outputs
    # Use QUERIES dictionary for consistent query strings
```

### 4. CSV Generation Function
```python
def generate_csv():
    ground_truth = get_ground_truth_function()
    
    eval_cases = [
        {
            "query": QUERIES["key1"],
            "expected_output": str(ground_truth["data1"]),
        },
        # etc. - NO strategy column
    ]
```

### 5. Main Block Structure
```python
if __name__ == "__main__":
    # Generate CSV
    generate_csv()
    
    # Run evaluation (can be commented out)
    # evaluate()
```

## Key Requirements

### Query Consistency
- Define query strings ONCE in `QUERIES` dictionary
- Use same queries in both dataset cases and CSV generation
- NO repetition of query strings

### No Strategy Column
- Remove `strategy` column from CSV generation
- Do not include strategy in eval_cases dictionaries

### Ground Truth Integration  
- Use actual calculated data for expected outputs
- Make dataset creation dynamic, not hardcoded
- Ensure ground truth function works with current data

### File Organization
- Import all necessary dependencies at top
- Keep Pydantic models for type validation
- Maintain same evaluation structure but with dynamic data

## Example Files
Reference [step1_evals.py](mdc:evals/scenario1/step1_evals.py) and [step2_evals.py](mdc:evals/scenario1/step2_evals.py) as completed examples of this pattern.

## Steps to Apply
1. Look at existing `stepN_data_filtering_cohort.py` for ground truth function
2. Look at existing `stepN_evals.py` for query patterns  
3. Look at existing `stepN_evaluation.csv` for expected structure
4. Apply consolidation pattern from step1 and step2 examples
5. Test that ground truth function produces correct data
6. Verify CSV generation works without strategy column
7. Clean up old files after consolidation

## Completed Files
- ✅ [step1_evals.py](mdc:evals/scenario1/step1_evals.py) - Customer cohort identification
- ✅ [step2_evals.py](mdc:evals/scenario1/step2_evals.py) - ARPU calculation

## Files to Process
- [step3_churn_rate_calculation.py](mdc:evals/scenario1/step3_churn_rate_calculation.py) → step3_evals.py  
- [step4_ltv_calculation.py](mdc:evals/scenario1/step4_ltv_calculation.py) → step4_evals.py
- [step5_cac_ltv_analysis.py](mdc:evals/scenario1/step5_cac_ltv_analysis.py) → step5_evals.py
