---
description:
globs:
alwaysApply: false
---
# Evaluation File Structure Rules

## File Naming Convention
- Use format: `{step_name}_evals.py` (e.g., `step1_evals.py`, `arman_evals.py`)
- CSV outputs: `{step_name}_evaluation.csv`

## Required Structure

### 1. Imports and Configuration
```python
import json
from pathlib import Path
import pandas as pd
from pydantic import BaseModel, Field
from pydantic_evals import Case, Dataset
from operate_ai.evals import EqEvaluator, Query, eval_task
```

### 2. Pydantic Models for Complex Outputs
Define specific output types for structured data:
```python
class CustomersPerSubscriptionType(BaseModel):
    monthly: int = Field(description="The number of customers with a monthly subscription.")
    annual: int = Field(description="The number of customers with an annual subscription.")
```

### 3. Ground Truth Function
- Name: `get_{domain}_ground_truth()` or similar descriptive name
- Returns: Dictionary with all calculated ground truth values
- Should be self-contained and deterministic

### 4. QUERIES Dictionary
- Store all evaluation queries in a centralized QUERIES dict
- Use descriptive keys that match the evaluation purpose
- Apply [eval-query-clarity.mdc](mdc:.cursor/rules/eval-query-clarity.mdc) rules

### 5. Dataset Creation Function
```python
def create_{step}_dataset(start_index: int | None = None, end_index: int | None = None):
    # Get ground truth
    ground_truth = get_ground_truth_function()
    
    # Create cases with proper typing
    cases = [Case(...)]
    
    # Return dataset with slicing support
    return Dataset[Query[ResultT], ResultT](cases=cases[start_index:end_index])
```

### 6. CSV Generation Function
```python
def generate_csv():
    # Create eval cases for CSV export
    # Save to proper path in evals/scenario1/
    # Include helpful print statements
```

### 7. Evaluation Runner
```python
def evaluate(workspace_name: str = "1", start_index: int | None = None, end_index: int | None = None):
    # Configure model with fallbacks
    # Setup workspace 
    # Run evaluation and print results
```

## Type Safety
- Use proper type hints: `type ResultT = int | CustomModel1 | CustomModel2`
- Define union types for all possible output types
- Use proper generic typing for Dataset creation

## Reference Files
- [evals/scenario1/step1_evals.py](mdc:evals/scenario1/step1_evals.py) - Primary reference implementation
- [evals/scenario1/step2_evals.py](mdc:evals/scenario1/step2_evals.py) - Complex ARPU calculations
- [evals/scenario1/arman_evals.py](mdc:evals/scenario1/arman_evals.py) - Advanced metrics

## Data Loading Pattern
```python
def load_data():
    # Load from operateai_scenario1_data directory
    # Convert date columns consistently
    # Return tuple of dataframes
```

Use this pattern for consistent data loading across evaluation files.
