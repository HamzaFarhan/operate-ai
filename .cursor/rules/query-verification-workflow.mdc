---
description: 
globs: 
alwaysApply: false
---
# Query Verification Workflow for OperateAI Evaluation Files

## Overview
This rule defines the process for verifying queries against data generated by [operateai_scenario1_data.py](mdc:operateai_scenario1_data.py). Use this workflow whenever creating or modifying ANY evaluation file in the `evals/` directory, including:

- [evals/scenario1/arman_evals.py](mdc:evals/scenario1/arman_evals.py)
- [evals/scenario1/step1_evals.py](mdc:evals/scenario1/step1_evals.py) 
- [evals/scenario1/step2_evals.py](mdc:evals/scenario1/step2_evals.py)
- [evals/scenario1/step3_evals.py](mdc:evals/scenario1/step3_evals.py)
- [evals/scenario1/step4_evals.py](mdc:evals/scenario1/step4_evals.py)
- [evals/scenario1/step5_evals.py](mdc:evals/scenario1/step5_evals.py)
- Any future evaluation files

## Verification Steps

### 1. Check for Hardcoded Values
- **Look for**: Hardcoded values like `1.0`, `0.0` in ground truth calculations
- **Verify**: All calculations are dynamic and based on actual data
- **Example Issue**: `"revenue_retention_jan_2023": 1.0` (hardcoded)
- **Solution**: Implement proper calculation logic using pandas operations

### 2. Validate Data Availability
Before implementing queries for specific time periods:

```bash
# Check what data exists for specific months/periods
uv run python -c "
import pandas as pd
df = pd.read_csv('operateai_scenario1_data/subscriptions.csv')
df['StartDate'] = pd.to_datetime(df['StartDate'])
annual = df[df['SubscriptionType'] == 'Annual']
print('Annual subscriptions by month:')
print(annual.groupby(annual['StartDate'].dt.to_period('M')).size())
"

# For other data types, check customers, orders, marketing_spend
uv run python -c "
import pandas as pd
customers = pd.read_csv('operateai_scenario1_data/customers.csv')
customers['AcquisitionDate'] = pd.to_datetime(customers['AcquisitionDate'])
print('Customer acquisitions by month:')
print(customers.groupby(customers['AcquisitionDate'].dt.to_period('M')).size().head(10))
"
```

### 3. Variable Name Consistency
- **Ensure**: Variable names match what they're actually calculating
- **Example**: If calculating August 2023 data, use `revenue_retention_aug_2023_12m` not `revenue_retention_jan_2023_12m`
- **Update**: Both the calculation variables AND the QUERIES dictionary keys
- **Update**: All references in dataset creation and CSV generation

### 4. Test Ground Truth Calculations
After implementing/modifying any evaluation file:

```bash
# Test specific calculations
uv run python -c "
from evals.scenario1.EVAL_FILE_NAME import calculate_FUNCTION_NAME
import json
gt = calculate_FUNCTION_NAME()
print(json.dumps(gt, indent=2, default=str))
"

# Test specific metrics only
uv run python -c "
from evals.scenario1.EVAL_FILE_NAME import calculate_FUNCTION_NAME
gt = calculate_FUNCTION_NAME()
for k, v in gt.items():
    if 'TARGET_METRIC' in k:
        print(f'{k}: {v}')
"
```

### 5. Debug Unexpected Values
When calculations return 0.0 or unexpected results:

```bash
# Create debug script to investigate data
cat > debug_eval.py << 'EOF'
import pandas as pd
from pathlib import Path

# Load data
data_dir = Path("operateai_scenario1_data")
subscriptions_df = pd.read_csv(data_dir / "subscriptions.csv")
orders_df = pd.read_csv(data_dir / "orders.csv")
customers_df = pd.read_csv(data_dir / "customers.csv")
marketing_spend_df = pd.read_csv(data_dir / "marketing_spend.csv")

# Convert dates
subscriptions_df["StartDate"] = pd.to_datetime(subscriptions_df["StartDate"])
orders_df["OrderDate"] = pd.to_datetime(orders_df["OrderDate"])
customers_df["AcquisitionDate"] = pd.to_datetime(customers_df["AcquisitionDate"])
marketing_spend_df["Date"] = pd.to_datetime(marketing_spend_df["Date"])

# Debug specific time period
target_month = "2023-08-01"  # Replace with your target
print(f"Investigating {target_month}")

# Check what data exists for that period
# Add your specific debugging logic here
EOF

uv run python debug_eval.py
rm debug_eval.py
```

### 6. Verify CSV Generation
Test CSV output after changes:

```bash
# Generate evaluation CSV
uv run python -c "
from evals.scenario1.EVAL_FILE_NAME import generate_csv
generate_csv()
"

# Check the output
head -5 evals/scenario1/EVAL_FILE_evaluation.csv
```

### 7. Final Verification
Run complete evaluation to ensure everything works:

```bash
# Test full evaluation workflow  
uv run python -c "
from evals.scenario1.EVAL_FILE_NAME import calculate_FUNCTION_NAME, create_DATASET_NAME
gt = calculate_FUNCTION_NAME()
dataset = create_DATASET_NAME()
print(f'Ground truth keys: {list(gt.keys())}')
print(f'Dataset cases: {len(dataset.cases)}')
"
```

## Common Issues & Solutions

### Issue: 0.0 Retention/Metrics
**Cause**: No data exists for specified time period
**Solution**: 
1. Check data availability (Step 2)
2. Use months that actually have data
3. Update query descriptions to match

### Issue: Variable Name Mismatches
**Cause**: Variable names don't reflect actual calculations
**Solution**:
1. Update calculation variable names
2. Update QUERIES dictionary keys
3. Update all dataset references
4. Update CSV generation code

### Issue: Linter Errors
**Cause**: Pandas operations have complex type annotations
**Solution**: These are expected and can be ignored if calculations work correctly

## Quick Commands Reference

```bash
# Check data for any time period
uv run python -c "import pandas as pd; df = pd.read_csv('operateai_scenario1_data/DATA_FILE.csv'); df['DATE_COLUMN'] = pd.to_datetime(df['DATE_COLUMN']); print(df.groupby(df['DATE_COLUMN'].dt.to_period('M')).size())"

# Test any evaluation function
uv run python -c "from evals.scenario1.EVAL_FILE import FUNCTION_NAME; print(FUNCTION_NAME())"

# Generate any evaluation CSV
uv run python -c "from evals.scenario1.EVAL_FILE import generate_csv; generate_csv()"

# Test specific metric calculations
uv run python -c "from evals.scenario1.EVAL_FILE import FUNCTION_NAME; result = FUNCTION_NAME(); print({k: v for k, v in result.items() if 'METRIC' in k})"
```

## File Templates
Replace placeholders when using:
- `EVAL_FILE_NAME` → actual evaluation file name (e.g., `arman_evals`, `step1_evals`)
- `FUNCTION_NAME` → ground truth function name (e.g., `calculate_arman_ground_truth`)
- `DATASET_NAME` → dataset creation function (e.g., `create_arman_dataset`)
- `DATA_FILE` → CSV file name (e.g., `subscriptions`, `orders`, `customers`)
- `DATE_COLUMN` → date column name (e.g., `StartDate`, `OrderDate`)
- `TARGET_METRIC` → specific metric to debug (e.g., `revenue_retention`)

## Notes
- All evaluation files use the same data from `operateai_scenario1_data/`
- Data is regenerated by [operateai_scenario1_data.py](mdc:operateai_scenario1_data.py)
- Always verify calculations are dynamic, not hardcoded
- Use actual months that have data in the dataset
- Variable names should clearly indicate what is being calculated
